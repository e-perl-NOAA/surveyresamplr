---
title: "A Simple Example"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{A Simple Example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
# set CRAN mirror
options(repos = c(CRAN = "https://cran.rstudio.com"))

# get rid of memory limits
options(future.globals.maxSize = 1 * 1024^4) # Allow up to 1 TB for globals
```

## Install and load libraries
```{r comment = FALSE, warning = FALSE, message = FALSE, error = FALSE, results='hide'}
library(surveyresamplr)

# Here we list all the packages we will need for this whole process
# We'll also use this in our works cited page.
p <- c(
  "here",
  "flextable",

  # tidyverse
  "dplyr",
  "tidyr",
  "viridis",
  "ggplot2",
  "tibble",
  "janitor",
  "data.table",

  # parallelizing
  "forcats",
  "purrr",
  "furrr",
  "doParallel",

  # modeling
  "arrow",
  "future.apply",
  "future.callr",
  "sdmTMB", # install.packages("remotes"",; remotes::install_github("pbs-assess/sdmTMBextra", dependencies = TRUE",
  "Matrix",
  "MASS",
  "cluster",
  "TMB",
  "INLA"
)

pkg_install <- function(p) {
  install.packages(p)
  require(p, character.only = TRUE)
}
base::lapply(p, pkg_install)
```

## Test Species List

Then we define our species and model list. In this simple example, we'll assess a model for eastern Bering Sea (EBS) walleye pollock (*Gadus chalcogrammus*) from 2015-present, using data collected by the Alaska Fisheries Science Center. 

Walleye pollock are fairly well distributed across the area, very common in the area, and very economically important, so this will be a great species to start testing these models with. 

```{r define-spp-list}
spp_list <- data.frame(
  srvy = "EBS",
  common_name = "walleye pollock",
  file_name = "simple_walleye_pollock",
  species_code = as.character(21740),
  filter_lat_lt = NA,
  filter_lat_gt = NA,
  filter_depth = NA,
  model_fn = "total_catch_wt_kg ~ 0 + factor(year)",
  model_family = "delta_gamma",
  model_anisotropy = TRUE,
  model_spatiotemporal = "iid, iid"
)
```

## Pull in data

Some example data is included in the R package for examples like this. We'll need 

1. Zero-filled catch data from the survey to use for developing the model and 
2. an extrapolation grid that you can replicate across years for your prediction matrix. Note that if your model models over depth (as with this model) your prediction grid will also need to have a depth field. 

Use `?surveyresamplr::noaa_afsc_catch` and `?surveyresamplr::noaa_afsc_ebs_pred_grid_depth`) to learn more about these data sources. In the meantime, this is what these resources look like: 

```{r explore-catch}
# ?surveyresamplr::noaa_afsc_catch

head(surveyresamplr::noaa_afsc_catch) |>
  flextable::flextable()
```


```{r explore-catch-plot, fig.width=6, fig.height=4, fig.cap= "CPUE (kg/km^2) of walleye pollock (Weight CPUE; kg/km2) from 2023 and 2024 in the EBS survey. "}
dat <- surveyresamplr::noaa_afsc_catch |>
  dplyr::filter(year %in% 2023:2024 &
    srvy %in% c("EBS") &
    species_code == 21740)

ggplot2::ggplot(
  data = dat |> dplyr::filter(cpue_kgkm2 != 0),
  mapping = aes(
    x = longitude_dd,
    y = latitude_dd,
    size = cpue_kgkm2
  )
) +
  ggplot2::geom_point(alpha = .75) +
  ggplot2::geom_point(
    data = dat |> dplyr::filter(cpue_kgkm2 %in% c(0, NA)),
    color = "red",
    shape = 17,
    alpha = .75,
    size = 3
  ) +
  ggplot2::xlab("Longitude °W") +
  ggplot2::ylab("Latitude °N") +
  ggplot2::ggtitle(
    label = "CPUE (kg/km^2) of walleye pollock (Weight CPUE; kg/km2)",
    subtitle = "AFSC Eastern Bering Sea bottom trawl survey"
  ) +
  ggplot2::scale_size_continuous(name = "Weight (kg)") +
  ggplot2::facet_wrap(facets = vars(year)) +
  ggplot2::theme_bw()
```


```{r explore-grid}
# ?surveyresamplr::noaa_afsc_ebs_pred_grid_depth

head(surveyresamplr::noaa_afsc_ebs_pred_grid_depth) |>
  flextable::flextable()
```

```{r explore-grid-plot, fig.width=6, fig.height=6, fig.cap= "Prediction grid for the EBS survey. "}
ggplot2::ggplot(
  data = surveyresamplr::noaa_afsc_ebs_pred_grid_depth,
  mapping = aes(
    x = longitude_dd,
    y = latitude_dd,
    color = depth_m
  )
) +
  ggplot2::geom_point(
    alpha = .5,
    size = .5
  ) +
  ggplot2::xlab("Longitude °W") +
  ggplot2::ylab("Latitude °N") +
  ggplot2::ggtitle(
    label = "Prediction Grid",
    subtitle = "AFSC Eastern Bering Sea bottom trawl survey"
  ) +
  ggplot2::scale_color_gradient(name = "Depth (m)") +
  ggplot2::theme_bw()
```   

Here we load the data for the model run, cropping it to the data we would like to include (the EBS survey and years greater than 2010) and replicating the the prediction grid across the years in the catch data. 

```{r load-data}
### Load survey data -----------------------------------------------------------

catch <- surveyresamplr::noaa_afsc_catch |>
  dplyr::filter(srvy == "EBS") |>
  dplyr::filter(year >= 2015)

### Load grid data -------------------------------------------------------------

grid_yrs <- sdmTMB::replicate_df(
  dat = surveyresamplr::noaa_afsc_ebs_pred_grid_depth,
  time_name = "year",
  time_values = unique(catch$year)
)
```

Now you'll notice the year column has been added, repeated with all of the years in the catch data. 

```{r grid-yrs}
head(grid_yrs) |>
  flextable::flextable()
```

Set variables. This... requires more description. `seq(from = seq_from, to = seq_to, by = seq_by)`

tot_dataframes = effort x replicates - (replicates - 1). TOLEDO: is this hard and fast?

```{r set-vars}
srvy <- "EBS"
seq_from <- 0.50
seq_to <- 1.0
seq_by <- 0.25
tot_dataframes <- 3
replicate_num <- 7
```

# Set directories

```{r set-directories}
wd <- paste0(here::here(), "/vignettes/")
dir_out <- paste0(wd, "output/")
dir_final <- paste0(dir_out, "EBS_simple_0results/")
dir.create(dir_final, showWarnings = FALSE)
crs_latlon <- "+proj=longlat +datum=WGS84" # decimal degrees
```

# Run resampling models

explain why `purrr::map` is important, what the sink files are for. 

```{r run-models, eval = FALSE}
start.time <- Sys.time()
sink(file = paste0(dir_final, srvy, "_simple_logfile.txt"), append = FALSE, split = TRUE)
purrr::map(
  1:nrow(spp_list),
  ~ clean_and_resample(
    spp_list[.x, ],
    catch, seq_from, seq_to, seq_by,
    tot_dataframes, replicate_num, grid_yrs, dir_out
  )
)
sink()
write.csv(x = data.frame(time = as.numeric(Sys.time() - start.time), units = units(Sys.time() - start.time)), file = paste0(dir_final, srvy, "_simple_time.csv"))
```

```{r time}
a <- read.csv(file = paste0(dir_final, srvy, "_simple_time.csv"))
print(paste0("Time difference of ", round(a$time, 2), " ", a$units))
```

```{r sink-results, eval = FALSE}
cat(readLines(con = paste0(dir_final, srvy, "_simple_logfile.txt")))
```

```{r sink-results-backup}
# EBS walleye pollock
# ...Starting parallel SDM processing
#
# ...05_1
#
# This mesh has > 1000 vertices. Mesh complexity has the single largest influence on fitting speed. Consider whether you require a mesh this complex, especially for initial model exploration.
# Check `your_mesh$mesh$n` to view the number of vertices.Warning: The model may not have converged: non-positive-definite Hessian matrix.Warning: The model may not have converged: non-positive-definite Hessian matrix.Warning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs produced✔ Non-linear minimizer suggests successful convergence
# ✖ Non-positive-definite Hessian matrix: model may not have converged
# ℹ Try simplifying the model, adjusting the mesh, or adding priors
#
# ✔ No extreme or very small eigenvalues detected
# ✔ No gradients with respect to fixed effects are >= 0.001
# Warning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs produced✔ No fixed-effect standard errors are NA
# Warning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs produced✖ `b_j` standard error may be large
# ℹ Try simplifying the model, adjusting the mesh, or adding priors
#
# ✖ `ln_kappa` standard error may be large
# ℹ `ln_kappa` is an internal parameter affecting `range`
# ℹ `range` is the distance at which data are effectively independent
# ℹ Try simplifying the model, adjusting the mesh, or adding priors
#
# Warning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs produced✔ No sigma parameters are < 0.01
# ✔ No sigma parameters are > 100
# ✔ Range parameters don't look unreasonably large
#
# ...05_2
#
# This mesh has > 1000 vertices. Mesh complexity has the single largest influence on fitting speed. Consider whether you require a mesh this complex, especially for initial model exploration.
# Check `your_mesh$mesh$n` to view the number of vertices.Warning: NaNs producedWarning: The model may not have converged: non-positive-definite Hessian matrix.Warning: NaNs producedWarning: The model may not have converged: non-positive-definite Hessian matrix.Warning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs produced✔ Non-linear minimizer suggests successful convergence
# ✖ Non-positive-definite Hessian matrix: model may not have converged
# ℹ Try simplifying the model, adjusting the mesh, or adding priors
#
# ✔ No extreme or very small eigenvalues detected
# ✖ `b_j2` gradient > 0.001
# ℹ See ?run_extra_optimization(), standardize covariates, and/or simplify the model
#
# ✖ `ln_tau_E` gradient > 0.001
# ℹ See ?run_extra_optimization(), standardize covariates, and/or simplify the model
#
# ✖ `ln_kappa` gradient > 0.001
# ℹ See ?run_extra_optimization(), standardize covariates, and/or simplify the model
#
# ✖ `ln_phi` gradient > 0.001
# ℹ See ?run_extra_optimization(), standardize covariates, and/or simplify the model
#
# Warning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs produced✔ No fixed-effect standard errors are NA
# Warning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs produced✖ `b_j` standard error may be large
# ℹ Try simplifying the model, adjusting the mesh, or adding priors
#
# Warning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs produced✔ No sigma parameters are < 0.01
# ✔ No sigma parameters are > 100
# ✔ Range parameters don't look unreasonably large
#
# ...05_3
#
# This mesh has > 1000 vertices. Mesh complexity has the single largest influence on fitting speed. Consider whether you require a mesh this complex, especially for initial model exploration.
# Check `your_mesh$mesh$n` to view the number of vertices.Warning: NaNs producedWarning: The model may not have converged: non-positive-definite Hessian matrix.Warning: NaNs producedWarning: The model may not have converged: non-positive-definite Hessian matrix.Warning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs produced✔ Non-linear minimizer suggests successful convergence
# ✖ Non-positive-definite Hessian matrix: model may not have converged
# ℹ Try simplifying the model, adjusting the mesh, or adding priors
#
# ✔ No extreme or very small eigenvalues detected
# ✖ `b_j2` gradient > 0.001
# ℹ See ?run_extra_optimization(), standardize covariates, and/or simplify the model
#
# ✖ `b_j2` gradient > 0.001
# ℹ See ?run_extra_optimization(), standardize covariates, and/or simplify the model
#
# ✖ `ln_tau_E` gradient > 0.001
# ℹ See ?run_extra_optimization(), standardize covariates, and/or simplify the model
#
# ✖ `ln_kappa` gradient > 0.001
# ℹ See ?run_extra_optimization(), standardize covariates, and/or simplify the model
#
# Warning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs produced✔ No fixed-effect standard errors are NA
# Warning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs produced✖ `b_j` standard error may be large
# ℹ Try simplifying the model, adjusting the mesh, or adding priors
#
# Warning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs producedWarning: NaNs produced✔ No sigma parameters are < 0.01
# ✔ No sigma parameters are > 100
# ✔ Range parameters don't look unreasonably large
# ...Parallel SDM processing complete
```

```{r results-run1, eval = TRUE}
out <- plot_results(srvy = paste0(srvy, "_simple"), dir_out = dir_out, dir_final = dir_final)
```

```{r results-run, eval = TRUE, echo = FALSE}
load(file = paste0(dir_final, "analysisoutput.rdata"))
```

```{r results-plots}
# load(file = paste0(dir_out, paste0(srvy, "_simple_0figures/figures.rdata")), verbose = TRUE)
out$plots
```

Parameter output: 

```{r results-tables-1}
i <- 1
print(names(out$tables)[i])
head(out$tables[i][[1]]) |>
  flextable::flextable()
```


```{r results-tables-2}
i <- 1 + i
print(names(out$tables)[i])
head(out$tables[i][[1]]) |>
  flextable::flextable()
```


```{r results-tables-3}
i <- 1 + i
print(names(out$tables)[i])
head(out$tables[i][[1]]) |>
  flextable::flextable()
```


```{r results-tables-4}
i <- 1 + i
print(names(out$tables)[i])
head(out$tables[i][[1]]) |>
  flextable::flextable()
```
